# MODEL PERFORMANCE & EVALUATION

nn_metrics = {
    'Training': {'MAE': 2357.38, 'RMSE': 4514.37, 'R2': 0.8593},
    'Validation': {'MAE': 2333.20, 'RMSE': 4507.66, 'R2': 0.8558},
    'Test': {'MAE': 2367.77, 'RMSE': 4536.41, 'R2': 0.8673}
}

dt_metrics = {
    'Training': {'MAE': 2457.11, 'RMSE': 4241.61, 'R2': 0.8757},
    'Validation': {'MAE': 2564.16, 'RMSE': 4720.49, 'R2': 0.8418},
    'Test': {'MAE': 2850.36, 'RMSE': 4747.02, 'R2': 0.8547}
}

rf_metrics = {
    'Training': {'MAE': 2015.00, 'RMSE': 4445.00, 'R2': 0.8597},
    'Validation': {'MAE': 2004.00, 'RMSE': 4433.00, 'R2': 0.8605},
    'Test': {'MAE': 2112.00, 'RMSE': 4551.00, 'R2': 0.8665}
}

# Create comparison DataFrame
comparison_data = []

for model_name, metrics in [('Neural Network', nn_metrics), 
                            ('Decision Tree', dt_metrics), 
                            ('Random Forest', rf_metrics)]:
    for dataset in ['Training', 'Validation', 'Test']:
        comparison_data.append({
            'Model': model_name,
            'Dataset': dataset,
            'MAE ($)': metrics[dataset]['MAE'],
            'RMSE ($)': metrics[dataset]['RMSE'],
            'R²': metrics[dataset]['R2']
        })

comparison_df = pd.DataFrame(comparison_data)
pivot_df = comparison_df.pivot(index='Model', columns='Dataset')

print("PERFORMANCE COMPARISON TABLE")
print("=" * 75)

metrics_order = ['MAE ($)', 'RMSE ($)', 'R²']
for metric in metrics_order:
    print(f"\n{metric}:")
    print("-" * 40)
    metric_df = pivot_df[metric]
    if metric in ['MAE ($)', 'RMSE ($)']:
        metric_df = metric_df.applymap(lambda x: f"${x:,.0f}")
    else:
        metric_df = metric_df.applymap(lambda x: f"{x:.4f}")
    
    print(metric_df.to_string())

# Overfitting/Underfitting Analysis
print("\nOVERFITTING/UNDERFITTING ANALYSIS")
print("-" * 50)

for model_name in ['Neural Network', 'Decision Tree', 'Random Forest']:
    model_data = comparison_df[comparison_df['Model'] == model_name]
    
    train_r2 = model_data[model_data['Dataset'] == 'Training']['R²'].values[0]
    val_r2 = model_data[model_data['Dataset'] == 'Validation']['R²'].values[0]
    test_r2 = model_data[model_data['Dataset'] == 'Test']['R²'].values[0]
    
    gap_train_val = train_r2 - val_r2
    gap_val_test = abs(val_r2 - test_r2)
    
    print(f"\n{model_name}:")
    print(f"  Training R²: {train_r2:.4f}")
    print(f"  Validation R²: {val_r2:.4f}")
    print(f"  Test R²: {test_r2:.4f}")
    print(f"  Train-Val Gap: {gap_train_val:.4f}")
    
    if gap_train_val > 0.05:
        print(f"  OVERFITTING: Large gap between train and validation")
    elif gap_train_val < 0.01:
        print(f"  GOOD GENERALIZATION: Small train-val gap")
    else:
        print(f"  MODERATE OVERFITTING: Moderate train-val gap")

# Visualizations
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# R² Comparison Bar Chart
ax1 = axes[0, 0]
models = ['Neural Network', 'Decision Tree', 'Random Forest']
test_r2_values = [nn_metrics['Test']['R2'], dt_metrics['Test']['R2'], rf_metrics['Test']['R2']]

colors = ['#3498db', '#2ecc71', '#e74c3c']
bars = ax1.bar(models, test_r2_values, color=colors, edgecolor='black', linewidth=1.5)

ax1.set_ylabel('R² Score')
ax1.set_title('Test Set R² Comparison')
ax1.set_ylim([0.8, 0.9])
ax1.axhline(y=0.85, color='gray', linestyle='--', alpha=0.7, label='Good (0.85)')
ax1.grid(True, alpha=0.3, axis='y')

for bar, val in zip(bars, test_r2_values):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2, height + 0.002,
            f'{val:.4f}', ha='center', va='bottom', fontsize=10)

ax1.legend()

# MAE Comparison
ax2 = axes[0, 1]
mae_values = [nn_metrics['Test']['MAE'], dt_metrics['Test']['MAE'], rf_metrics['Test']['MAE']]
bars2 = ax2.bar(models, mae_values, color=colors, edgecolor='black', linewidth=1.5)

ax2.set_ylabel('MAE ($)')
ax2.set_title('Test Set MAE Comparison (Lower is Better)')
ax2.grid(True, alpha=0.3, axis='y')

for bar, val in zip(bars2, mae_values):
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2, height + 50,
            f'${val:,.0f}', ha='center', va='bottom', fontsize=10)

# Overfitting Analysis
ax3 = axes[1, 0]
gap_data = []
for model_name in models:
    if model_name == 'Neural Network':
        gap = nn_metrics['Training']['R2'] - nn_metrics['Validation']['R2']
    elif model_name == 'Decision Tree':
        gap = dt_metrics['Training']['R2'] - dt_metrics['Validation']['R2']
    else:
        gap = rf_metrics['Training']['R2'] - rf_metrics['Validation']['R2']
    gap_data.append(gap)

bars3 = ax3.bar(models, gap_data, color=colors, edgecolor='black', linewidth=1.5)
ax3.set_ylabel('R² Gap (Train - Val)')
ax3.set_title('Overfitting Analysis (Smaller gap = better)')
ax3.axhline(y=0, color='black', linewidth=1)
ax3.axhline(y=0.02, color='orange', linestyle='--', alpha=0.7, label='Acceptable (0.02)')
ax3.grid(True, alpha=0.3, axis='y')

for bar, val in zip(bars3, gap_data):
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2, height + 0.002 if height > 0 else height - 0.003,
            f'{val:.4f}', ha='center', va='bottom' if height > 0 else 'top', 
            fontsize=10)

ax3.legend()

# Performance Summary Heatmap
ax4 = axes[1, 1]
heatmap_data = []
for model in models:
    if model == 'Neural Network':
        heatmap_data.append([nn_metrics['Training']['R2'], nn_metrics['Validation']['R2'], nn_metrics['Test']['R2']])
    elif model == 'Decision Tree':
        heatmap_data.append([dt_metrics['Training']['R2'], dt_metrics['Validation']['R2'], dt_metrics['Test']['R2']])
    else:
        heatmap_data.append([rf_metrics['Training']['R2'], rf_metrics['Validation']['R2'], rf_metrics['Test']['R2']])

heatmap_data = pd.DataFrame(heatmap_data, index=models, columns=['Train', 'Validation', 'Test'])
sns.heatmap(heatmap_data, annot=True, fmt='.4f', cmap='YlOrRd', 
            cbar_kws={'label': 'R² Score'}, ax=ax4, vmin=0.8, vmax=0.9)
ax4.set_title('R² Performance Heatmap')
ax4.set_xlabel('Dataset')
ax4.set_ylabel('Model')

plt.tight_layout()
plt.savefig('model_performance_evaluation.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nVisualizations saved as 'model_performance_evaluation.png'")

# Final comparison table
summary_df = pd.DataFrame({
    'Model': models,
    'Test R²': [f"{nn_metrics['Test']['R2']:.4f}", 
                f"{dt_metrics['Test']['R2']:.4f}", 
                f"{rf_metrics['Test']['R2']:.4f}"],
    'Test MAE': [f"${nn_metrics['Test']['MAE']:,.0f}", 
                 f"${dt_metrics['Test']['MAE']:,.0f}", 
                 f"${rf_metrics['Test']['MAE']:,.0f}"],
    'Test RMSE': [f"${nn_metrics['Test']['RMSE']:,.0f}", 
                  f"${dt_metrics['Test']['RMSE']:,.0f}", 
                  f"${rf_metrics['Test']['RMSE']:,.0f}"],
    'Train-Val Gap': [f"{nn_metrics['Training']['R2'] - nn_metrics['Validation']['R2']:.4f}", 
                      f"{dt_metrics['Training']['R2'] - dt_metrics['Validation']['R2']:.4f}", 
                      f"{rf_metrics['Training']['R2'] - rf_metrics['Validation']['R2']:.4f}"]
})

print("\nFINAL COMPARISON TABLE:")
print("-" * 80)
print(summary_df.to_string(index=False))
print("-" * 80)

print(f"\nBEST PERFORMING MODEL: Random Forest")
print(f"Test R²: {rf_metrics['Test']['R2']:.4f}")
print(f"Test MAE: ${rf_metrics['Test']['MAE']:,.0f}")
