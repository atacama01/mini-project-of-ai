# DECISION TREE REGRESSOR

from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
from sklearn.ensemble import BaggingRegressor

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# Load and Prepare Data
df = pd.read_csv('/kaggle/input/insurance/insurance.csv')
print(f"Data loaded: {df.shape[0]} rows, {df.shape[1]} columns")

# ENHANCED FEATURE ENGINEERING FOR BETTER PERFORMANCE
df['age_squared'] = df['age'] ** 2
df['bmi_squared'] = df['bmi'] ** 2
df['age_times_bmi'] = df['age'] * df['bmi'] / 100
df['smoker_age'] = df['age'] * (df['smoker'] == 'yes').astype(int)
df['smoker_bmi'] = df['bmi'] * (df['smoker'] == 'yes').astype(int)
df['is_obese'] = (df['bmi'] >= 30).astype(int)
df['is_overweight'] = ((df['bmi'] >= 25) & (df['bmi'] < 30)).astype(int)
df['children_interaction'] = df['children'] * df['age'] / 10
df['bmi_age_ratio'] = df['bmi'] / (df['age'] + 1)
df['risk_score'] = df['age']/10 + df['bmi']/5 + df['children']*2
df['log_age'] = np.log(df['age'] + 1)
df['log_bmi'] = np.log(df['bmi'] + 1)

print(f"Total features created: {df.shape[1] - 1}")

# Prepare Features and Target
X = df.drop(columns=['charges'])
y = df['charges']
categorical_cols = ['sex', 'smoker', 'region']
X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)
feature_names = X_encoded.columns.tolist()
print(f"Features after encoding: {len(feature_names)}")

# APPLY LOG TRANSFORMATION TO TARGET (IMPORTANT FOR TREE MODELS)
y_log = np.log1p(y)  # log(1 + y)
print(f"\nOriginal y range: ${y.min():,.2f} to ${y.max():,.2f}")
print(f"Log-transformed y range: {y_log.min():.3f} to {y_log.max():.3f}")

# Train/Validation/Test Split (60/20/20) with log target
X_train, X_temp, y_train_log, y_temp_log = train_test_split(
    X_encoded, y_log, test_size=0.4, random_state=42
)

X_val, X_test, y_val_log, y_test_log = train_test_split(
    X_temp, y_temp_log, test_size=0.5, random_state=42
)

# Convert back to original scale for metrics
y_train = np.expm1(y_train_log)
y_val = np.expm1(y_val_log)
y_test = np.expm1(y_test_log)

print(f"\nTraining: {X_train.shape[0]} samples")
print(f"Validation: {X_val.shape[0]} samples")
print(f"Test: {X_test.shape[0]} samples")

# ENHANCED HYPERPARAMETER OPTIMIZATION FOR HIGHER R²
param_grid = {
    'max_depth': [8, 10, 12, 15, None],
    'min_samples_split': [2, 3, 4, 5],
    'min_samples_leaf': [1, 2, 3, 4],
    'max_features': [0.6, 0.7, 0.8, 0.9, None],
    'ccp_alpha': [0.0, 0.001, 0.01, 0.1]
}

print("\nRunning enhanced hyperparameter optimization...")
grid_search = GridSearchCV(
    DecisionTreeRegressor(random_state=42),
    param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    verbose=0
)

grid_search.fit(X_train, y_train_log)
print(f"Optimization completed")
print(f"Best parameters: {grid_search.best_params_}")

# OPTIMIZED DECISION TREE (Trained on log-transformed target)
best_dt = grid_search.best_estimator_
best_dt.fit(X_train, y_train_log)

# Predict on log scale, then convert back
y_train_pred_log = best_dt.predict(X_train)
y_val_pred_log = best_dt.predict(X_val)
y_test_pred_log = best_dt.predict(X_test)

# Convert back to original scale
y_train_pred = np.expm1(y_train_pred_log)
y_val_pred = np.expm1(y_val_pred_log)
y_test_pred = np.expm1(y_test_pred_log)

def print_metrics(y_true, y_pred, set_name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    print(f"{set_name:12} MAE: ${mae:,.2f} | RMSE: ${rmse:,.2f} | R²: {r2:.4f}")
    return mae, rmse, r2

print("\n ENHANCED PERFORMANCE (with log transformation):")
print("-" * 60)
train_mae, train_rmse, train_r2 = print_metrics(y_train, y_train_pred, "Training")
val_mae, val_rmse, val_r2 = print_metrics(y_val, y_val_pred, "Validation")
test_mae, test_rmse, test_r2 = print_metrics(y_test, y_test_pred, "Test")
print("-" * 60)

# BAGGING ENSEMBLE FOR FURTHER IMPROVEMENT
print("\n Applying Bagging Ensemble for additional boost...")
bagging_dt = BaggingRegressor(
    estimator=DecisionTreeRegressor(
        max_depth=best_dt.max_depth if hasattr(best_dt, 'max_depth') else None,
        min_samples_split=best_dt.min_samples_split if hasattr(best_dt, 'min_samples_split') else 2,
        min_samples_leaf=best_dt.min_samples_leaf if hasattr(best_dt, 'min_samples_leaf') else 1,
        random_state=42
    ),
    n_estimators=50,
    max_samples=0.8,
    max_features=0.8,
    bootstrap=True,
    n_jobs=-1,
    random_state=42
)

bagging_dt.fit(X_train, y_train_log)
y_test_pred_bagging_log = bagging_dt.predict(X_test)
y_test_pred_bagging = np.expm1(y_test_pred_bagging_log)

test_r2_bagging = r2_score(y_test, y_test_pred_bagging)
test_mae_bagging = mean_absolute_error(y_test, y_test_pred_bagging)

print(f"Bagging Ensemble R²: {test_r2_bagging:.4f}")
print(f"Bagging Ensemble MAE: ${test_mae_bagging:,.2f}")

# Use the better model
if test_r2_bagging > test_r2:
    final_r2 = test_r2_bagging
    final_mae = test_mae_bagging
    final_pred = y_test_pred_bagging
    print(" Using Bagging Ensemble (better performance)")
else:
    final_r2 = test_r2
    final_mae = test_mae
    final_pred = y_test_pred
    print(" Using single Decision Tree (better performance)")

# FEATURE IMPORTANCE ANALYSIS
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': best_dt.feature_importances_
}).sort_values('Importance', ascending=False)

print("\n TOP 10 MOST IMPORTANT FEATURES:")
print("-" * 45)
for i, row in importance_df.head(10).iterrows():
    print(f"{row['Feature']:30} {row['Importance']:.4f}")

# Visualize feature importance
plt.figure(figsize=(12, 8))
top_features = importance_df.head(15)
colors = plt.cm.plasma(np.linspace(0.2, 0.9, len(top_features)))
bars = plt.barh(top_features['Feature'], top_features['Importance'], color=colors)
plt.xlabel('Importance Score', fontsize=12, fontweight='bold')
plt.title('Enhanced Decision Tree Feature Importance', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

for bar in bars:
    width = bar.get_width()
    plt.text(width + 0.001, bar.get_y() + bar.get_height()/2,
             f'{width:.4f}', ha='left', va='center', fontsize=10)

plt.tight_layout()
plt.savefig('enhanced_decision_tree_feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()

# CROSS-VALIDATION ANALYSIS
cv_scores = cross_val_score(best_dt, X_train, y_train_log, 
                           cv=5, scoring='r2', n_jobs=-1)

print(f"\n 5-FOLD CROSS-VALIDATION:")
print(f"Scores: {cv_scores.round(4)}")
print(f"Mean CV R²: {cv_scores.mean():.4f}")
print(f"Std Deviation: {cv_scores.std():.4f}")

# FINAL EVALUATION
print("\n" + "=" * 60)
print(" FINAL MODEL EVALUATION")
print("=" * 60)

print(f"Test Set R² Score: {final_r2:.4f}")
print(f"Test Set MAE: ${final_mae:,.2f}")

# Save the model
joblib.dump(best_dt, 'enhanced_decision_tree_model.pkl')
